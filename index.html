<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Atharva Kulkarni</title> <meta name="author" content="Atharva Kulkarni"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9B%B0%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://athrvkk.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/outreach/">Outreach &amp; Mentorship</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="text-align: justify"> <div class="post"> <header class="post-header"> <img src="/assets/img/usc-viterbi-logo.png" alt="click here" height="58px" width="200px" style="float:right"> <h1 class="post-title"> <span class="font-weight-bold">Atharva</span> Kulkarni </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/malta_profile-480.webp 480w, /assets/img/malta_profile-800.webp 800w, /assets/img/malta_profile-1400.webp 1400w, " sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/malta_profile.jpeg?888bb7480f7ce86b3840c7985f6b0ce4" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="malta_profile.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix" style="text-align: justify"> <p>Hello! I am a first year CS PhD student at the <a href="https://www.usc.edu" rel="external nofollow noopener" target="_blank">University of Southern California</a>, advised by <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank">Swabha Swayamdipta</a>, and a member of <a href="https://nlp.usc.edu" rel="external nofollow noopener" target="_blank">USC NLP Group</a> and <a href="https://dill-lab.github.io" rel="external nofollow noopener" target="_blank">DILL Lab</a>. Currently, I am a visiting student researcher at <a href="https://simons.berkeley.edu/programs/special-year-large-language-models-transformers-part-2" rel="external nofollow noopener" target="_blank">UC Berkeley ‚Äì Simons Institute for the Theory of Computing</a>.</p> <p>My current research focuses on <strong>theoretical &amp; empirical understanding of language models</strong>. Specifically, I am interested in:</p> <ul> <li>Building a principled understanding of when &amp; why they work / fail.</li> <li>Understanding the science behind their scaling &amp; generalization capabilities.</li> <li>Exploring avenues for improving their reliability &amp; trustworthiness.</li> </ul> <p>I recently graduated with a <a href="https://www.lti.cs.cmu.edu/academics/masters-programs/mlt.html" rel="external nofollow noopener" target="_blank">Masters in Language Technologies (MLT)</a> from <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a> ‚Äì <a href="https://www.lti.cs.cmu.edu/" rel="external nofollow noopener" target="_blank">Language Technologies Institute</a>, where I was fortunate to be advised by <a href="https://www.cs.cmu.edu/~bapoczos/" rel="external nofollow noopener" target="_blank">Barnab√°s P√≥czos</a> &amp; <a href="http://www.phontron.com" rel="external nofollow noopener" target="_blank">Graham Neubig</a>. During my masters, I interned at <a href="https://machinelearning.apple.com" rel="external nofollow noopener" target="_blank">Apple</a> twice (summer ‚Äò23 &amp; ‚Äò24) with the Siri &amp; Information Intelligence group. </p> <p>Before coming to CMU, I was a Predoctoral Researcher (Research Associate) at the <a href="https://www.lcs2.in/" rel="external nofollow noopener" target="_blank">Laboratory for Computational Social Systems (LCS2), IIIT Delhi</a>. Even before that, I graduated with a Bachelors of Engineering in Computer Science from <a href="https://http://www.unipune.ac.in" rel="external nofollow noopener" target="_blank">Savitribai Phule Pune University</a>.</p> <p>I‚Äôm eager to connect with my academic peers! If our research interests align (or diverge) in intriguing ways, I‚Äôd be delighted to explore potential collaborations or simply exchange ideas! </p> <p>I am also a strong advocate for diversity and mentorship in computer science research. If you‚Äôd like to chat about navigating ML/NLP research, grad school applications, or want to collaborate on a research project, please visit the <a href="./outreach">Outreach &amp; Mentorship</a> tab for more information.</p> </div> <hr style="border-top: 1px solid #ccc;"> <h2 style="color: inherit;">News</h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width:10%">Jan 2025</th> <td> Attending <a href="https://simons.berkeley.edu/programs/special-year-large-language-models-transformers-part-2" rel="external nofollow noopener" target="_blank">UC Berkeley ‚Äì Simons Institute for the Theory of Computing</a> as a visiting PhD student for the <a href="https://simons.berkeley.edu/programs/special-year-large-language-models-transformers-part-2" rel="external nofollow noopener" target="_blank">Special Year on Large Language Models and Transformers Progam (Part 2)</a> üêª! </td> </tr> <tr> <th scope="row" style="width:10%">Sep 2024</th> <td> Finally our work on <a href="">evaluating LLMs on comorbid mental mealth issues</a> is out! It will be present at <a href="https://2024.emnlp.org" rel="external nofollow noopener" target="_blank">EMNLP 2024 main conference</a> <img class="emoji" title=":us:" alt=":us:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f1fa-1f1f8.png" height="20" width="20">! </td> </tr> <tr> <th scope="row" style="width:10%">Aug 2024</th> <td> Started my PhD at USC! Excited to work with <a href="https://swabhs.com" rel="external nofollow noopener" target="_blank">Prof. Swabha Swayamdipta</a> and the amazing members of <a href="https://dill-lab.github.io" rel="external nofollow noopener" target="_blank">DILL Lab</a> &amp; <a href="https://nlp.usc.edu" rel="external nofollow noopener" target="_blank">USC NLP Group</a>. </td> </tr> </table> </div> <a href="/news/"> üï∞Ô∏è all news ...</a> </div> <hr style="border-top: 1px solid #ccc;"> <h2 style="color: inherit;">Selected Publications</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">TMLR</abbr></div> <div id="kulkarni2024multitask" class="col-sm-8"> <div class="title">Multitask Learning Can Improve Worst-Group Outcomes</div> <div class="author"> <em>Atharva Kulkarni<sup>*</sup></em>,¬†Lucio M. Dery<sup>*</sup>,¬†Amrith Setlur,¬†Aditi Raghunathan,¬†Ameet Talwalkar,¬†and¬†Graham Neubig</div> <div class="periodical"> <em>NeurIPS 2023 SSLTheoryPractice Workshop <br> Transactions on Machine Learning Research</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=sPlhAIp6mk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=sPlhAIp6mk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/athrvkk/MTL-group-robustness" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model‚Äôs average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the standard setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not consistently, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) ‚Äì a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language processing datasets and find that our regularized MTL approach consistently outperforms JTT on both average and worst-group outcomes. Our official code can be found here: https://github.com/athrvkk/MTL-group-robustness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EACL</abbr></div> <div id="kulkarni-etal-2024-synthdst" class="col-sm-8"> <div class="title">SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking</div> <div class="author"> <em>Atharva Kulkarni</em>,¬†Bo-Hsiang Tseng,¬†Joel Moniz,¬†Dhivya Piraviperumal,¬†Hong Yu,¬†and¬†Shruti Bhargava</div> <div class="periodical"> <em>In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.eacl-long.120" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2024.eacl-long.120.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/apple/ml-synthdst" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, ‚Äò\textitCan we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?‚Äô Addressing this question, we propose , a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from results in 4-5% improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly 98% of the performance compared to the few-shot setup using human-annotated training data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SIGKDD</abbr></div> <div id="10.1145/3580305.3599896" class="col-sm-8"> <div class="title">Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment</div> <div class="author"> <em>Atharva Kulkarni<sup>*</sup></em>,¬†Sarah Masud<sup>*</sup>,¬†Vikram Goyal,¬†and¬†Tanmoy Chakraborty</div> <div class="periodical"> <em>In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3580305.3599896" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3580305.3599896" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/LCS2-IIITD/GotHate" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Social media is awash with hateful content, much of which is often veiled with linguistic and topical diversity. The benchmark datasets used for hate speech detection do not account for such divagation as they are predominantly compiled using hate lexicons. However, capturing hate signals becomes challenging in neutrally-seeded malicious content. Thus, designing models and datasets that mimic the real-world variability of hate warrants further investigation.To this end, we present GOTHate, a large-scale code-mixed crowdsourced dataset of around 51k posts for hate speech detection from Twitter. GOTHate is neutrally seeded, encompassing different languages and topics. We conduct detailed comparisons of GOTHate with the existing hate speech datasets, highlighting its novelty. We benchmark it with 10 recent baselines. Our extensive empirical and benchmarking experiments suggest that GOTHate is hard to classify in a text-only setup. Thus, we investigate how adding endogenous signals enhances the hate speech detection task. We augment GOTHate with the user‚Äôs timeline information and ego network, bringing the overall data source closer to the real-world setup for understanding hateful content. Our proposed solution HEN-mBERT is a modular, multilingual, mixture-of-experts model that enriches the linguistic subspace with latent endogenous signals from history, topology, and exemplars. HEN-mBERT transcends the best baseline by 2.5% and 5% in overall macro-F1 and hate class F1, respectively. Inspired by our experiments, in partnership with Wipro AI, we are developing a semi-automated pipeline to detect hateful content as a part of their mission to tackle online harm.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IJCAI</abbr></div> <div id="ijcai2023p709" class="col-sm-8"> <div class="title">Learning and Reasoning Multifaceted and Longitudinal Data for Poverty Estimates and Livelihood Capabilities of Lagged Regions in Rural India</div> <div class="author"> <em>Atharva Kulkarni</em>,¬†Raya Das,¬†Ravi S. Srivastava,¬†and¬†Tanmoy Chakraborty</div> <div class="periodical"> <em>In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23</em>, Aug 2023 </div> <div class="periodical"> AI for Good - Projects </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ijcai.org/proceedings/2023/709" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.ijcai.org/proceedings/2023/0709.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Poverty is a multifaceted phenomenon linked to the lack of capabilities of households to earn a sustainable livelihood, increasingly being assessed using multidimensional indicators. Its spatial pattern depends on social, economic, political, and regional variables. Artificial intelligence has shown immense scope in analyzing the complexities and nuances of poverty. The proposed project aims to examine the poverty situation of rural India for the period of 1990-2022 based on the quality of life and livelihood indicators. The districts will be classified into ‚Äòadvanced‚Äô, ‚Äòcatching up‚Äô, ‚Äòfalling behind‚Äô, and ‚Äòlagged‚Äô regions. The project proposes to integrate multiple data sources, including conventional national-level large sample household surveys, census surveys, and proxy variables like daytime, and nighttime data from satellite images, and communication networks, to name a few, to provide a comprehensive view of poverty at the district level. The project also intends to examine causation and longitudinal analysis to examine the reasons for poverty. Poverty and inequality could be widening in developing countries due to demographic and growth-agglomerating policies. Therefore, targeting the lagging regions and the vulnerable population is essential to eradicate poverty and improve the quality of life to achieve the goal of ‚Äòzero poverty‚Äô. Thus, the study also focuses on the districts with a higher share of the marginal section of the population compared to the national average to trace the performance of development indicators and their association with poverty in these regions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="sundriyal-etal-2022-empowering" class="col-sm-8"> <div class="title">Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter</div> <div class="author"> <em>Atharva Kulkarni<sup>*</sup></em>,¬†Megha Sundriyal<sup>*</sup>,¬†Vaibhav Pulastya,¬†Md. Shad Akhtar,¬†and¬†Tanmoy Chakraborty</div> <div class="periodical"> <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.emnlp-main.525" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2022.emnlp-main.525.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/LCS2-IIITD/DABERTA-EMNLP-2022" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The widespread diffusion of medical and political claims in the wake of COVID-19 has led to a voluminous rise in misinformation and fake news. The current vogue is to employ manual fact-checkers to efficiently classify and verify such data to combat this avalanche of claim-ridden misinformation. However, the rate of information dissemination is such that it vastly outpaces the fact-checkers‚Äô strength. Therefore, to aid manual fact-checkers in eliminating the superfluous content, it becomes imperative to automatically identify and extract the snippets of claim-worthy (mis)information present in a post. In this work, we introduce the novel task of Claim Span Identification (CSI). We propose CURT, a large-scale Twitter corpus with token-level claim spans on more than 7.5k tweets. Furthermore, along with the standard token classification baselines, we benchmark our dataset with DABERTa, an adapter-based variation of RoBERTa. The experimental results attest that DABERTa outperforms the baseline systems across several evaluation metrics, improving by about 1.5 points. We also report detailed error analysis to validate the model‚Äôs performance along with the ablation studies. Lastly, we release our comprehensive span annotation guidelines for public use.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="kumar-etal-2022-become" class="col-sm-8"> <div class="title">When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues</div> <div class="author"> <em>Atharva Kulkarni<sup>*</sup></em>,¬†Shivani Kumar<sup>*</sup>,¬†Md Shad Akhtar,¬†and¬†Tanmoy Chakraborty</div> <div class="periodical"> <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.acl-long.411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2022.acl-long.411.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/lcs2-iiitd/maf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication. While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication. Though sarcasm identification has been a well-explored topic in dialogue analysis, for conversational systems to truly grasp a conversation‚Äôs innate meaning and generate appropriate responses, simply detecting sarcasm is not enough; it is vital to explain its underlying sarcastic connotation to capture its true essence. In this work, we study the discourse structure of sarcastic conversations and propose a novel task ‚Äì Sarcasm Explanation in Dialogue (SED). Set in a multimodal and code-mixed setting, the task aims to generate natural language explanations of satirical conversations. To this end, we curate WITS, a new dataset to support our task. We propose MAF (Modality Aware Fusion), a multimodal context-aware attention and global information fusion module to capture multimodality and use it to benchmark WITS. The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics. Lastly, we carry out detailed analysis both quantitatively and qualitatively.</p> </div> </div> </div> </li> </ol> </div> <hr style="border-top: 1px solid #ccc;"> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%74%68%61%72%76%61.%6B%75%6C%6B%61%72%6E%69@%75%73%63.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=tQTAiXwAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/1992797499" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/athrvkk" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/athrvkk" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/athrvkk" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"> Slack (USC / CMU) or email is usually the best way to reach me! </div> </div> </article> </div> </div> <footer class="fixed-bottom" style="text-align: center"> <div class="container mt-0" style="text-align: center"> ¬© Copyright 2025 Atharva Kulkarni. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>